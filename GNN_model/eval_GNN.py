import torch
import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score
from typing import Dict
from torch_geometric.data import HeteroData
from config import Config
from tqdm import tqdm


class GNNEvaluator:
    def __init__(self, model: torch.nn.Module, graph: HeteroData, eval_set: str, config: Config):
        """
        Args:
            model: trained GNN model
            graph: PyG HeteroData graph (needed for full user/item embeddings)
            device: cpu or cuda
        """
        self.device = config.gnn.device
        self.model = model.to(self.device)  # model lives on GPU for training
        self.scores_path = getattr(config.paths, f"{eval_set}_scores_file")
        self.top_k = config.gnn.k_hit
        self.eval_batch_size = config.gnn.eval_batch_size

        self.num_users = graph['user'].num_nodes
        self.num_items = graph['item'].num_nodes

        # Cache for embeddings
        self._cached_embeddings = None
        self._orig_id_to_graph_idx = None

        # Pre-load and process ground truth relevance scores
        self.ground_truth = {}
        self.eval_user_indices = np.array([])
        self._load_and_process_ground_truth()

    def _load_and_process_ground_truth(self):
        """Load the pre-computed scores AND pre-process into a dict."""
        df = pd.read_parquet(self.scores_path)
        df = df.rename(columns={"user_id": "user_idx", "item_id": "item_idx"})

        # Create a fast lookup dictionary for ground truth
        self.ground_truth = {}

        # Group by user_idx
        for uid, group in tqdm(df.groupby('user_idx'), desc="Processing GT"):
            self.ground_truth[uid] = {
                "item_idx": group["item_idx"].values,  # Original item IDs
                "adjusted_score": group["adjusted_score"].values,
                "seen_in_train": group["seen_in_train"].values.astype(bool)
            }

        self.eval_user_indices = np.array(list(self.ground_truth.keys()), dtype=np.int64)

    def _get_embeddings(self):
        """
        Run full-graph propagation once on CPU (to avoid OOM)
        and cache embeddings.
        """
        if self._cached_embeddings is None:
            # --- REVERTED TO CPU FORWARD PASS ---
            # Move model to CPU temporarily
            self.model.to('cpu')
            self.model.eval()
            with torch.no_grad():
                # Use the CPU-based forward pass
                user_emb_cpu, item_emb_cpu, _ = self.model.forward_cpu()
                # Cache them on the CPU
                self._cached_embeddings = (user_emb_cpu.cpu(), item_emb_cpu.cpu())

            # Move model back to GPU for training
            self.model.to(self.device)
            # --- END REVERT ---

            if self._orig_id_to_graph_idx is None:
                # Get the original IDs tensor (size=num_items) stored in the model
                item_orig_ids_tensor = self.model.item_original_ids.cpu()

                # Create a mapping from original_id -> graph_idx
                self._orig_id_to_graph_idx = {
                    orig_id.item(): graph_idx
                    for graph_idx, orig_id in enumerate(item_orig_ids_tensor)
                }

        return self._cached_embeddings

    def _pre_map_ground_truth(self, mapper):
        """
        Uses the embedding mapper to convert all original item IDs
        in the ground truth dict to the model's graph indices.
        This is done once to avoid repeated .map() calls.
        """
        for uid in tqdm(self.eval_user_indices, desc="Mapping GT"):
            gt = self.ground_truth[uid]
            orig_ids = gt["item_idx"]

            valid_indices = []
            graph_indices = []

            for i, orig_id in enumerate(orig_ids):
                graph_idx = mapper.get(orig_id)
                if graph_idx is not None:
                    valid_indices.append(i)
                    graph_indices.append(graph_idx)

            # Store the mapped and filtered ground truth
            gt["graph_idx"] = np.array(graph_indices, dtype=np.int64)
            gt["valid_adj_scores"] = gt["adjusted_score"][valid_indices]
            gt["valid_seen"] = gt["seen_in_train"][valid_indices]

    def evaluate(self) -> Dict[str, float]:
        """
        Batched, GPU-accelerated evaluation.
        """
        print(">>> Starting evaluation (batched, GPU)...")
        k = self.top_k

        # 1. Get Embeddings (on CPU) and Mapper
        user_emb_cpu, item_emb_cpu = self._get_embeddings()
        mapper = self._orig_id_to_graph_idx

        # 2. Pre-map all ground truth item IDs
        self._pre_map_ground_truth(mapper)

        # --- MOVE FULL ITEM TENSOR TO GPU (ONCE) ---
        item_emb_gpu = item_emb_cpu.to(self.device)

        # 3. Initialize metrics
        metrics = {
            "ndcg@k": [],
            "hit_like@k": [],
            "hit_like_listen@k": [],
            "auc": [],
            "dislike_fpr@k": [],
            "novelty@k": []
        }

        # 4. Process users in batches
        for i in tqdm(range(0, len(self.eval_user_indices), self.eval_batch_size), desc="Evaluating batches"):
            # Get user indices for this batch
            batch_user_indices = self.eval_user_indices[i: i + self.eval_batch_size]

            # --- MOVE SMALL USER BATCH TO GPU ---
            batch_user_emb_gpu = user_emb_cpu[batch_user_indices].to(self.device)

            # --- Perform scoring for all users in batch on GPU ---
            # (eval_batch_size, D) @ (D, num_items) -> (eval_batch_size, num_items)
            batch_scores_gpu = torch.mm(batch_user_emb_gpu, item_emb_gpu.T)

            # --- Get Top-K on GPU ---
            _, batch_topk_indices_gpu = torch.topk(batch_scores_gpu, k, dim=-1)

            # --- Move *only* the small results to CPU ---
            batch_topk_indices_cpu = batch_topk_indices_gpu.cpu().numpy()

            # We need the full score list for AUC, so move that too.
            batch_scores_cpu = batch_scores_gpu.cpu().numpy()

            # 5. Calculate metrics for each user in the batch (on CPU)
            for j, user_idx in enumerate(batch_user_indices):
                topk_idx = batch_topk_indices_cpu[j]
                topk_set = set(topk_idx)

                # Get pre-processed ground truth
                gt = self.ground_truth[user_idx]
                gt_items = gt["graph_idx"]  # Already mapped
                gt_adj = gt["valid_adj_scores"]  # Already filtered

                # --- NDCG@k (graded) ---
                # Build a sparse relevance vector for this user
                relevance = np.zeros(self.num_items, dtype=float)
                relevance[gt_items] = gt_adj

                top_rel = relevance[topk_idx]
                dcg = np.sum((2 ** np.maximum(top_rel, 0) - 1) / np.log2(np.arange(2, k + 2)))
                ideal = np.sort(np.maximum(gt_adj, 0))[::-1][:k]
                idcg = np.sum((2 ** ideal - 1) / np.log2(np.arange(2, len(ideal) + 2)))
                ndcg = dcg / (idcg if idcg > 0 else 1.0)
                metrics["ndcg@k"].append(ndcg)

                # --- Hit@k (like-equivalent) ---
                like_items = gt_items[gt_adj > 1.0]  # >1 â‰ˆ explicit like
                metrics["hit_like@k"].append(float(len(set(like_items) & topk_set) > 0))

                # --- Hit@k (like+listen) ---
                pos_items = gt_items[gt_adj > 0.5]
                metrics["hit_like_listen@k"].append(float(len(set(pos_items) & topk_set) > 0))

                # --- AUC (pos vs neg) ---
                pos_items_auc = gt_items[gt_adj > 0]
                neg_items_auc = gt_items[gt_adj < 0]

                if len(pos_items_auc) > 0 and len(neg_items_auc) > 0:
                    y_true = np.concatenate([
                        np.ones(len(pos_items_auc)),
                        np.zeros(len(neg_items_auc))
                    ])

                    # Get the full score vector for this user
                    pred_scores_user_cpu = batch_scores_cpu[j]

                    y_score = np.concatenate([
                        pred_scores_user_cpu[pos_items_auc],
                        pred_scores_user_cpu[neg_items_auc]
                    ])

                    try:
                        metrics["auc"].append(roc_auc_score(y_true, y_score))
                    except ValueError:
                        pass  # Handle cases where all scores are identical

                # --- Dislike FPR@k ---
                dislike_items = gt_items[gt_adj < 0]
                if len(dislike_items):
                    metrics["dislike_fpr@k"].append(float(len(set(dislike_items) & topk_set) > 0))

                # --- Novelty@k (fraction unseen) ---
                all_interacted_items_set = set(gt_items)
                unseen_in_topk = sum(1 for i in topk_idx if i not in all_interacted_items_set)
                metrics["novelty@k"].append(unseen_in_topk / k)

        # ---- average over users ----
        print(">>> Evaluation complete.")
        return {m: float(np.mean(v)) if len(v) else 0.0 for m, v in metrics.items()}